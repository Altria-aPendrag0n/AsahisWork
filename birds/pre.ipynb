{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import sklearn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "import os\n",
    "import demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device : cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Current Device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDataset(Dataset):\n",
    "    def __init__(self, dataset_path, transform_fn, enhance_path=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform_fn\n",
    "        self.label_idx2name = {}\n",
    "        self.img_path = []\n",
    "        if dataset_path:\n",
    "            file_list = os.listdir(dataset_path)\n",
    "            file_list = file_list[0:20]\n",
    "            self.label_idx2name = np.array(file_list)\n",
    "            self.label_name2idx = {}\n",
    "            self.img2label = {}\n",
    "            for i in range(len(file_list)):\n",
    "                self.label_name2idx[self.label_idx2name[i]] = i\n",
    "                lst = glob.glob(f\"{dataset_path}/{file_list[i]}/*.jpg\")\n",
    "                if len(lst) >= 200:\n",
    "                    lst = lst[0:200]\n",
    "                self.img_path.extend(lst)\n",
    "                for j in range(len(lst)):\n",
    "                    self.img2label[lst[j]] = i\n",
    "        if enhance_path:\n",
    "            file_list = os.listdir(dataset_path)\n",
    "            file_list = file_list[0:20]\n",
    "            for i in range(len(file_list)):\n",
    "                lst = glob.glob(f\"{dataset_path}/{file_list[i]}/*.jpg\")\n",
    "                self.img_path.extend(lst)\n",
    "                for j in range(len(lst)):\n",
    "                    self.img2label[lst[j]] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.img_path[index]\n",
    "        label = self.img2label[img]\n",
    "        img = Image.open(img).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_mean = torch.Tensor([0.485, 0.456, 0.406])\n",
    "channel_std = torch.Tensor([0.229, 0.224, 0.225])\n",
    "transformations_list = []\n",
    "\n",
    "vit_train_transform_fn = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.6),\n",
    "        transforms.RandomRotation(degrees=(30)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=0.3),\n",
    "        transforms.RandomAdjustSharpness(sharpness_factor=20, p=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=channel_mean, std=channel_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vit_valid_transform_fn = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=channel_mean, std=channel_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集图片的个数为：6480\n",
      "测试集图片的个数为：100\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CatDataset(\n",
    "    dataset_path=\"./train\",\n",
    "    transform_fn=vit_train_transform_fn,\n",
    "    enhance_path=\"./generate\",\n",
    ")\n",
    "valid_dataset = CatDataset(dataset_path=\"./valid\", transform_fn=vit_train_transform_fn)\n",
    "valid_dataset.transform = vit_valid_transform_fn\n",
    "print(f\"训练集图片的个数为：{len(train_dataset)}\")\n",
    "print(f\"测试集图片的个数为：{len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainViT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PretrainViT, self).__init__()\n",
    "        model = demo.ViT(n_classes=20)\n",
    "        # num_classifier_feature = model.heads.head.in_features\n",
    "        # model.heads.head = nn.Sequential(nn.Linear(num_classifier_feature, 70))\n",
    "        self.model = model\n",
    "\n",
    "        for param in self.model.named_parameters():\n",
    "            if \"heads\" not in param[0]:\n",
    "                param[1].requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of paramaters: 0\n"
     ]
    }
   ],
   "source": [
    "net = PretrainViT()\n",
    "net.to(device)\n",
    "print(\n",
    "    f\"number of paramaters: {sum([param.numel() for param in net.parameters() if param.requires_grad])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.015)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.015)\n",
    "# optimizer = optim.RMSprop(net.parameters(), lr=0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, label):\n",
    "    output = output.to(\"cpu\")\n",
    "    label = label.to(\"cpu\")\n",
    "\n",
    "    sm = F.softmax(output, dim=1)\n",
    "    _, index = torch.max(sm, dim=1)\n",
    "    return torch.sum((label == index)) / label.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    for batch_idx, (batch_img, batch_label) in enumerate(dataloader):\n",
    "\n",
    "        batch_img = batch_img.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_img)\n",
    "        loss = criterion(output, batch_label)\n",
    "        loss.requires_grad_(True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        acc = get_accuracy(output, batch_label)\n",
    "        running_acc += acc\n",
    "        total_acc += acc\n",
    "\n",
    "        if batch_idx % 100 == 0 and batch_idx != 0:\n",
    "            print(\n",
    "                f\"[step: {batch_idx:4d}/{len(dataloader)}] loss: {running_loss / 100:.3f}\"\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    for batch_idx, (batch_img, batch_label) in enumerate(dataloader):\n",
    "\n",
    "        batch_img = batch_img.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        output = model(batch_img)\n",
    "        loss = criterion(output, batch_label)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        acc = get_accuracy(output, batch_label)\n",
    "        total_acc += acc\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  100/1620] loss: 3.154\n",
      "[step:  200/1620] loss: 3.164\n",
      "[step:  300/1620] loss: 3.116\n",
      "[step:  400/1620] loss: 3.184\n",
      "[step:  500/1620] loss: 3.182\n",
      "[step:  600/1620] loss: 3.173\n",
      "[step:  700/1620] loss: 3.100\n",
      "[step:  800/1620] loss: 3.162\n",
      "[step:  900/1620] loss: 3.132\n",
      "[step: 1000/1620] loss: 3.124\n",
      "[step: 1100/1620] loss: 3.143\n",
      "[step: 1200/1620] loss: 3.186\n",
      "[step: 1300/1620] loss: 3.134\n",
      "[step: 1400/1620] loss: 3.128\n",
      "[step: 1500/1620] loss: 3.101\n",
      "[step: 1600/1620] loss: 3.142\n",
      "Epoch:  0, training loss: 3.144, training acc: 0.053 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.157\n",
      "[step:  200/1620] loss: 3.165\n",
      "[step:  300/1620] loss: 3.144\n",
      "[step:  400/1620] loss: 3.132\n",
      "[step:  500/1620] loss: 3.131\n",
      "[step:  600/1620] loss: 3.160\n",
      "[step:  700/1620] loss: 3.137\n",
      "[step:  800/1620] loss: 3.115\n",
      "[step:  900/1620] loss: 3.148\n",
      "[step: 1000/1620] loss: 3.161\n",
      "[step: 1100/1620] loss: 3.144\n",
      "[step: 1200/1620] loss: 3.168\n",
      "[step: 1300/1620] loss: 3.183\n",
      "[step: 1400/1620] loss: 3.120\n",
      "[step: 1500/1620] loss: 3.163\n",
      "[step: 1600/1620] loss: 3.140\n",
      "Epoch:  1, training loss: 3.146, training acc: 0.053 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.221\n",
      "[step:  200/1620] loss: 3.209\n",
      "[step:  300/1620] loss: 3.151\n",
      "[step:  400/1620] loss: 3.146\n",
      "[step:  500/1620] loss: 3.157\n",
      "[step:  600/1620] loss: 3.134\n",
      "[step:  700/1620] loss: 3.125\n",
      "[step:  800/1620] loss: 3.108\n",
      "[step:  900/1620] loss: 3.171\n",
      "[step: 1000/1620] loss: 3.153\n",
      "[step: 1100/1620] loss: 3.140\n",
      "[step: 1200/1620] loss: 3.143\n",
      "[step: 1300/1620] loss: 3.109\n",
      "[step: 1400/1620] loss: 3.106\n",
      "[step: 1500/1620] loss: 3.148\n",
      "[step: 1600/1620] loss: 3.139\n",
      "Epoch:  2, training loss: 3.145, training acc: 0.051 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.188\n",
      "[step:  200/1620] loss: 3.176\n",
      "[step:  300/1620] loss: 3.163\n",
      "[step:  400/1620] loss: 3.185\n",
      "[step:  500/1620] loss: 3.119\n",
      "[step:  600/1620] loss: 3.159\n",
      "[step:  700/1620] loss: 3.137\n",
      "[step:  800/1620] loss: 3.144\n",
      "[step:  900/1620] loss: 3.164\n",
      "[step: 1000/1620] loss: 3.187\n",
      "[step: 1100/1620] loss: 3.109\n",
      "[step: 1200/1620] loss: 3.107\n",
      "[step: 1300/1620] loss: 3.150\n",
      "[step: 1400/1620] loss: 3.119\n",
      "[step: 1500/1620] loss: 3.131\n",
      "[step: 1600/1620] loss: 3.096\n",
      "Epoch:  3, training loss: 3.144, training acc: 0.052 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.170\n",
      "[step:  200/1620] loss: 3.165\n",
      "[step:  300/1620] loss: 3.141\n",
      "[step:  400/1620] loss: 3.147\n",
      "[step:  500/1620] loss: 3.121\n",
      "[step:  600/1620] loss: 3.181\n",
      "[step:  700/1620] loss: 3.152\n",
      "[step:  800/1620] loss: 3.157\n",
      "[step:  900/1620] loss: 3.180\n",
      "[step: 1000/1620] loss: 3.208\n",
      "[step: 1100/1620] loss: 3.121\n",
      "[step: 1200/1620] loss: 3.170\n",
      "[step: 1300/1620] loss: 3.077\n",
      "[step: 1400/1620] loss: 3.173\n",
      "[step: 1500/1620] loss: 3.089\n",
      "[step: 1600/1620] loss: 3.124\n",
      "Epoch:  4, training loss: 3.146, training acc: 0.052 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.193\n",
      "[step:  200/1620] loss: 3.128\n",
      "[step:  300/1620] loss: 3.140\n",
      "[step:  400/1620] loss: 3.150\n",
      "[step:  500/1620] loss: 3.190\n",
      "[step:  600/1620] loss: 3.176\n",
      "[step:  700/1620] loss: 3.159\n",
      "[step:  800/1620] loss: 3.190\n",
      "[step:  900/1620] loss: 3.133\n",
      "[step: 1000/1620] loss: 3.143\n",
      "[step: 1100/1620] loss: 3.144\n",
      "[step: 1200/1620] loss: 3.127\n",
      "[step: 1300/1620] loss: 3.106\n",
      "[step: 1400/1620] loss: 3.091\n",
      "[step: 1500/1620] loss: 3.087\n",
      "[step: 1600/1620] loss: 3.158\n",
      "Epoch:  5, training loss: 3.144, training acc: 0.052 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.144\n",
      "[step:  200/1620] loss: 3.129\n",
      "[step:  300/1620] loss: 3.181\n",
      "[step:  400/1620] loss: 3.119\n",
      "[step:  500/1620] loss: 3.171\n",
      "[step:  600/1620] loss: 3.168\n",
      "[step:  700/1620] loss: 3.140\n",
      "[step:  800/1620] loss: 3.062\n",
      "[step:  900/1620] loss: 3.180\n",
      "[step: 1000/1620] loss: 3.128\n",
      "[step: 1100/1620] loss: 3.168\n",
      "[step: 1200/1620] loss: 3.142\n",
      "[step: 1300/1620] loss: 3.133\n",
      "[step: 1400/1620] loss: 3.187\n",
      "[step: 1500/1620] loss: 3.155\n",
      "[step: 1600/1620] loss: 3.142\n",
      "Epoch:  6, training loss: 3.144, training acc: 0.053 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.211\n",
      "[step:  200/1620] loss: 3.143\n",
      "[step:  300/1620] loss: 3.144\n",
      "[step:  400/1620] loss: 3.192\n",
      "[step:  500/1620] loss: 3.175\n",
      "[step:  600/1620] loss: 3.117\n",
      "[step:  700/1620] loss: 3.144\n",
      "[step:  800/1620] loss: 3.093\n",
      "[step:  900/1620] loss: 3.122\n",
      "[step: 1000/1620] loss: 3.160\n",
      "[step: 1100/1620] loss: 3.175\n",
      "[step: 1200/1620] loss: 3.181\n",
      "[step: 1300/1620] loss: 3.143\n",
      "[step: 1400/1620] loss: 3.118\n",
      "[step: 1500/1620] loss: 3.129\n",
      "[step: 1600/1620] loss: 3.119\n",
      "Epoch:  7, training loss: 3.146, training acc: 0.053 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.186\n",
      "[step:  200/1620] loss: 3.160\n",
      "[step:  300/1620] loss: 3.115\n",
      "[step:  400/1620] loss: 3.152\n",
      "[step:  500/1620] loss: 3.162\n",
      "[step:  600/1620] loss: 3.165\n",
      "[step:  700/1620] loss: 3.130\n",
      "[step:  800/1620] loss: 3.128\n",
      "[step:  900/1620] loss: 3.166\n",
      "[step: 1000/1620] loss: 3.149\n",
      "[step: 1100/1620] loss: 3.173\n",
      "[step: 1200/1620] loss: 3.146\n",
      "[step: 1300/1620] loss: 3.160\n",
      "[step: 1400/1620] loss: 3.112\n",
      "[step: 1500/1620] loss: 3.154\n",
      "[step: 1600/1620] loss: 3.113\n",
      "Epoch:  8, training loss: 3.145, training acc: 0.052 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.162\n",
      "[step:  200/1620] loss: 3.154\n",
      "[step:  300/1620] loss: 3.163\n",
      "[step:  400/1620] loss: 3.131\n",
      "[step:  500/1620] loss: 3.131\n",
      "[step:  600/1620] loss: 3.115\n",
      "[step:  700/1620] loss: 3.128\n",
      "[step:  800/1620] loss: 3.144\n",
      "[step:  900/1620] loss: 3.141\n",
      "[step: 1000/1620] loss: 3.127\n",
      "[step: 1100/1620] loss: 3.154\n",
      "[step: 1200/1620] loss: 3.125\n",
      "[step: 1300/1620] loss: 3.161\n",
      "[step: 1400/1620] loss: 3.155\n",
      "[step: 1500/1620] loss: 3.177\n",
      "[step: 1600/1620] loss: 3.173\n",
      "Epoch:  9, training loss: 3.145, training acc: 0.052 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.164\n",
      "[step:  200/1620] loss: 3.128\n",
      "[step:  300/1620] loss: 3.177\n",
      "[step:  400/1620] loss: 3.133\n",
      "[step:  500/1620] loss: 3.165\n",
      "[step:  600/1620] loss: 3.104\n",
      "[step:  700/1620] loss: 3.078\n",
      "[step:  800/1620] loss: 3.110\n",
      "[step:  900/1620] loss: 3.188\n",
      "[step: 1000/1620] loss: 3.172\n",
      "[step: 1100/1620] loss: 3.130\n",
      "[step: 1200/1620] loss: 3.158\n",
      "[step: 1300/1620] loss: 3.198\n",
      "[step: 1400/1620] loss: 3.155\n",
      "[step: 1500/1620] loss: 3.132\n",
      "[step: 1600/1620] loss: 3.152\n",
      "Epoch: 10, training loss: 3.145, training acc: 0.051 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.154\n",
      "[step:  200/1620] loss: 3.179\n",
      "[step:  300/1620] loss: 3.142\n",
      "[step:  400/1620] loss: 3.149\n",
      "[step:  500/1620] loss: 3.121\n",
      "[step:  600/1620] loss: 3.142\n",
      "[step:  700/1620] loss: 3.118\n",
      "[step:  800/1620] loss: 3.152\n",
      "[step:  900/1620] loss: 3.169\n",
      "[step: 1000/1620] loss: 3.142\n",
      "[step: 1100/1620] loss: 3.199\n",
      "[step: 1200/1620] loss: 3.116\n",
      "[step: 1300/1620] loss: 3.159\n",
      "[step: 1400/1620] loss: 3.119\n",
      "[step: 1500/1620] loss: 3.165\n",
      "[step: 1600/1620] loss: 3.106\n",
      "Epoch: 11, training loss: 3.144, training acc: 0.054 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.174\n",
      "[step:  200/1620] loss: 3.140\n",
      "[step:  300/1620] loss: 3.138\n",
      "[step:  400/1620] loss: 3.142\n",
      "[step:  500/1620] loss: 3.156\n",
      "[step:  600/1620] loss: 3.159\n",
      "[step:  700/1620] loss: 3.167\n",
      "[step:  800/1620] loss: 3.186\n",
      "[step:  900/1620] loss: 3.126\n",
      "[step: 1000/1620] loss: 3.128\n",
      "[step: 1100/1620] loss: 3.122\n",
      "[step: 1200/1620] loss: 3.152\n",
      "[step: 1300/1620] loss: 3.147\n",
      "[step: 1400/1620] loss: 3.109\n",
      "[step: 1500/1620] loss: 3.160\n",
      "[step: 1600/1620] loss: 3.135\n",
      "Epoch: 12, training loss: 3.144, training acc: 0.052 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.155\n",
      "[step:  200/1620] loss: 3.119\n",
      "[step:  300/1620] loss: 3.141\n",
      "[step:  400/1620] loss: 3.123\n",
      "[step:  500/1620] loss: 3.168\n",
      "[step:  600/1620] loss: 3.151\n",
      "[step:  700/1620] loss: 3.121\n",
      "[step:  800/1620] loss: 3.111\n",
      "[step:  900/1620] loss: 3.171\n",
      "[step: 1000/1620] loss: 3.161\n",
      "[step: 1100/1620] loss: 3.156\n",
      "[step: 1200/1620] loss: 3.165\n",
      "[step: 1300/1620] loss: 3.104\n",
      "[step: 1400/1620] loss: 3.181\n",
      "[step: 1500/1620] loss: 3.151\n",
      "[step: 1600/1620] loss: 3.140\n",
      "Epoch: 13, training loss: 3.144, training acc: 0.052 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.152\n",
      "[step:  200/1620] loss: 3.166\n",
      "[step:  300/1620] loss: 3.142\n",
      "[step:  400/1620] loss: 3.090\n",
      "[step:  500/1620] loss: 3.156\n",
      "[step:  600/1620] loss: 3.126\n",
      "[step:  700/1620] loss: 3.148\n",
      "[step:  800/1620] loss: 3.142\n",
      "[step:  900/1620] loss: 3.136\n",
      "[step: 1000/1620] loss: 3.134\n",
      "[step: 1100/1620] loss: 3.126\n",
      "[step: 1200/1620] loss: 3.126\n",
      "[step: 1300/1620] loss: 3.125\n",
      "[step: 1400/1620] loss: 3.162\n",
      "[step: 1500/1620] loss: 3.188\n",
      "[step: 1600/1620] loss: 3.203\n",
      "Epoch: 14, training loss: 3.145, training acc: 0.052 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.136\n",
      "[step:  200/1620] loss: 3.165\n",
      "[step:  300/1620] loss: 3.124\n",
      "[step:  400/1620] loss: 3.122\n",
      "[step:  500/1620] loss: 3.157\n",
      "[step:  600/1620] loss: 3.151\n",
      "[step:  700/1620] loss: 3.197\n",
      "[step:  800/1620] loss: 3.130\n",
      "[step:  900/1620] loss: 3.093\n",
      "[step: 1000/1620] loss: 3.160\n",
      "[step: 1100/1620] loss: 3.131\n",
      "[step: 1200/1620] loss: 3.116\n",
      "[step: 1300/1620] loss: 3.171\n",
      "[step: 1400/1620] loss: 3.187\n",
      "[step: 1500/1620] loss: 3.134\n",
      "[step: 1600/1620] loss: 3.163\n",
      "Epoch: 15, training loss: 3.144, training acc: 0.053 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.142\n",
      "[step:  200/1620] loss: 3.204\n",
      "[step:  300/1620] loss: 3.117\n",
      "[step:  400/1620] loss: 3.099\n",
      "[step:  500/1620] loss: 3.154\n",
      "[step:  600/1620] loss: 3.149\n",
      "[step:  700/1620] loss: 3.153\n",
      "[step:  800/1620] loss: 3.145\n",
      "[step:  900/1620] loss: 3.181\n",
      "[step: 1000/1620] loss: 3.134\n",
      "[step: 1100/1620] loss: 3.145\n",
      "[step: 1200/1620] loss: 3.102\n",
      "[step: 1300/1620] loss: 3.128\n",
      "[step: 1400/1620] loss: 3.163\n",
      "[step: 1500/1620] loss: 3.180\n",
      "[step: 1600/1620] loss: 3.141\n",
      "Epoch: 16, training loss: 3.145, training acc: 0.053 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.232\n",
      "[step:  200/1620] loss: 3.137\n",
      "[step:  300/1620] loss: 3.117\n",
      "[step:  400/1620] loss: 3.186\n",
      "[step:  500/1620] loss: 3.133\n",
      "[step:  600/1620] loss: 3.150\n",
      "[step:  700/1620] loss: 3.119\n",
      "[step:  800/1620] loss: 3.123\n",
      "[step:  900/1620] loss: 3.115\n",
      "[step: 1000/1620] loss: 3.127\n",
      "[step: 1100/1620] loss: 3.158\n",
      "[step: 1200/1620] loss: 3.159\n",
      "[step: 1300/1620] loss: 3.149\n",
      "[step: 1400/1620] loss: 3.113\n",
      "[step: 1500/1620] loss: 3.174\n",
      "[step: 1600/1620] loss: 3.124\n",
      "Epoch: 17, training loss: 3.144, training acc: 0.052 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.211\n",
      "[step:  200/1620] loss: 3.089\n",
      "[step:  300/1620] loss: 3.148\n",
      "[step:  400/1620] loss: 3.133\n",
      "[step:  500/1620] loss: 3.146\n",
      "[step:  600/1620] loss: 3.130\n",
      "[step:  700/1620] loss: 3.112\n",
      "[step:  800/1620] loss: 3.181\n",
      "[step:  900/1620] loss: 3.163\n",
      "[step: 1000/1620] loss: 3.184\n",
      "[step: 1100/1620] loss: 3.134\n",
      "[step: 1200/1620] loss: 3.104\n",
      "[step: 1300/1620] loss: 3.129\n",
      "[step: 1400/1620] loss: 3.164\n",
      "[step: 1500/1620] loss: 3.104\n",
      "[step: 1600/1620] loss: 3.189\n",
      "Epoch: 18, training loss: 3.142, training acc: 0.052 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.171\n",
      "[step:  200/1620] loss: 3.135\n",
      "[step:  300/1620] loss: 3.177\n",
      "[step:  400/1620] loss: 3.167\n",
      "[step:  500/1620] loss: 3.109\n",
      "[step:  600/1620] loss: 3.125\n",
      "[step:  700/1620] loss: 3.182\n",
      "[step:  800/1620] loss: 3.151\n",
      "[step:  900/1620] loss: 3.086\n",
      "[step: 1000/1620] loss: 3.134\n",
      "[step: 1100/1620] loss: 3.127\n",
      "[step: 1200/1620] loss: 3.221\n",
      "[step: 1300/1620] loss: 3.107\n",
      "[step: 1400/1620] loss: 3.148\n",
      "[step: 1500/1620] loss: 3.130\n",
      "[step: 1600/1620] loss: 3.181\n",
      "Epoch: 19, training loss: 3.145, training acc: 0.053 validation loss: 3.180, validation acc: 0.020\n",
      "[step:  100/1620] loss: 3.112\n",
      "[step:  200/1620] loss: 3.112\n",
      "[step:  300/1620] loss: 3.164\n",
      "[step:  400/1620] loss: 3.164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> 8\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     valid_loss, valid_acc \u001b[38;5;241m=\u001b[39m validate(net, valid_dataloader)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, training acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, validation acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[50], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 20\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     23\u001b[0m acc \u001b[38;5;241m=\u001b[39m get_accuracy(output, batch_label)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.to(device)\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "train_acc_history = []\n",
    "valid_acc_history = []\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(net, train_dataloader)\n",
    "    valid_loss, valid_acc = validate(net, valid_dataloader)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    train_acc_history.append(train_acc)\n",
    "    valid_acc_history.append(valid_acc)\n",
    "\n",
    "    if valid_loss <= min(valid_loss_history):\n",
    "        torch.save(net.state_dict(), \"net.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
