{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:30:25.188897Z","iopub.status.busy":"2024-05-14T15:30:25.188533Z","iopub.status.idle":"2024-05-14T15:30:25.456862Z","shell.execute_reply":"2024-05-14T15:30:25.455567Z","shell.execute_reply.started":"2024-05-14T15:30:25.188867Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import Tensor\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import models\n","from tqdm import tqdm\n","import numpy as np\n","import os\n","import math \n","import glob\n","import pandas as pd\n","from PIL import Image \n","from PIL import ImageFile\n","from PIL import ImageEnhance\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","import sklearn\n","from torchvision import transforms\n","from torch.utils.data import Dataset,Subset,DataLoader\n","import matplotlib.pyplot as plt\n","import pickle\n","from sklearn.model_selection import train_test_split\n","from torchvision.transforms import Compose, Resize, ToTensor\n","import random\n","import timm\n","import torchvision.transforms as transforms\n","from einops.layers.torch import Rearrange\n","from timm.models.efficientnet_blocks import SqueezeExcite, DepthwiseSeparableConv\n","from typing import Type\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:30:20.188447Z","iopub.status.busy":"2024-05-14T15:30:20.188055Z","iopub.status.idle":"2024-05-14T15:30:20.218659Z","shell.execute_reply":"2024-05-14T15:30:20.217338Z","shell.execute_reply.started":"2024-05-14T15:30:20.188417Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Current Device : cuda\n"]}],"source":["torch.manual_seed(2023)\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(f'Current Device : {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.193787Z","iopub.status.idle":"2024-05-14T15:28:17.194131Z","shell.execute_reply":"2024-05-14T15:28:17.193981Z","shell.execute_reply.started":"2024-05-14T15:28:17.193967Z"},"trusted":true},"outputs":[],"source":["enhance_1 = transforms.Compose([ transforms.RandomHorizontalFlip(p=0.6),\n","  transforms.RandomRotation(degrees=(30))])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.195703Z","iopub.status.idle":"2024-05-14T15:28:17.197078Z","shell.execute_reply":"2024-05-14T15:28:17.196845Z","shell.execute_reply.started":"2024-05-14T15:28:17.196825Z"},"trusted":true},"outputs":[],"source":["class BirdDataset(Dataset):\n","    def __init__(self,dataset_path,transform_fn,enhance_path = None):\n","        self.dataset_path = dataset_path\n","        self.transform = transform_fn\n","        self.label_idx2name = {}\n","        self.img_path = []\n","        if dataset_path:\n","            file_list = os.listdir(dataset_path)\n","            file_list = file_list[0:73]\n","            self.label_idx2name = np.array(file_list)\n","            self.label_name2idx = {}\n","            self.img2label = {}\n","            for i in range(len(file_list)):                 \n","                self.label_name2idx[self.label_idx2name[i]] = i\n","                lst = glob.glob(f\"{dataset_path}/{file_list[i]}/*.jpg\")\n","                if len(lst)>=200:\n","                    lst = lst[0:200]\n","                self.img_path.extend(lst)\n","                for j in range(len(lst)):\n","                    self.img2label[lst[j]] = i\n","                \n","    def __len__(self):\n","        return len(self.img_path)\n","    \n","    def __getitem__(self,index):\n","        img =  self.img_path[index]\n","        label = self.img2label[img]\n","        img = Image.open(img).convert(\"RGB\"\n","        img = self.transform(img)\n","        return (img,label)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.198244Z","iopub.status.idle":"2024-05-14T15:28:17.198714Z","shell.execute_reply":"2024-05-14T15:28:17.198511Z","shell.execute_reply.started":"2024-05-14T15:28:17.198492Z"},"trusted":true},"outputs":[],"source":["channel_mean = torch.Tensor([0.485,0.456,0.406])\n","channel_std = torch.Tensor([0.229,0.224,0.225])\n","\n","vit_train_transform_fn = transforms.Compose([\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.RandomHorizontalFlip(p=0.6),\n","    transforms.RandomRotation(degrees=(30)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=channel_mean, std=channel_std)\n","]) \n","\n","train_transfrom_noaug_fn = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=channel_mean, std=channel_std)\n","])\n","\n","vit_valid_transform_fn = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=channel_mean, std=channel_std)\n","]) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.200508Z","iopub.status.idle":"2024-05-14T15:28:17.200976Z","shell.execute_reply":"2024-05-14T15:28:17.200772Z","shell.execute_reply.started":"2024-05-14T15:28:17.200752Z"},"trusted":true},"outputs":[],"source":["train_dataset = BirdDataset(dataset_path='../input/100-bird-species/train',transform_fn=vit_train_transform_fn)\n","valid_dataset = BirdDataset(dataset_path='../input/100-bird-species/valid',transform_fn=vit_valid_transform_fn)\n","valid_dataset.transform = vit_valid_transform_fn\n","print(f\"训练集图片的个数为：{len(train_dataset)}\")\n","print(f\"测试集图片的个数为：{len(valid_dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.202242Z","iopub.status.idle":"2024-05-14T15:28:17.202794Z","shell.execute_reply":"2024-05-14T15:28:17.202564Z","shell.execute_reply.started":"2024-05-14T15:28:17.202545Z"},"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=16,\n","    shuffle=True\n",")\n","\n","valid_dataloader = DataLoader(\n","    valid_dataset,\n","    batch_size=16,\n","    shuffle=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.204384Z","iopub.status.idle":"2024-05-14T15:28:17.204841Z","shell.execute_reply":"2024-05-14T15:28:17.204623Z","shell.execute_reply.started":"2024-05-14T15:28:17.204603Z"},"trusted":true},"outputs":[],"source":["def show_samples(batch_img, batch_label=None, num_samples=16):\n","\n","    sample_idx = 0\n","    total_col = 4\n","    total_row = math.ceil(num_samples / 4)\n","    col_idx = 0\n","    row_idx = 0\n","\n","    fig, axs = plt.subplots(total_row, total_col, figsize=(12, 12))\n","\n","    while sample_idx < num_samples:\n","        img = batch_img[sample_idx]\n","        img = img.view(3, -1) * channel_std.view(3, -1) + channel_mean.view(3, -1)\n","        img = img.view(3, 224, 224)\n","        img = img.permute(1, 2, 0)\n","        axs[row_idx, col_idx].imshow(img)\n","\n","        if batch_label != None:\n","            axs[row_idx, col_idx].set_title(train_dataset.label_idx2name[(batch_label[sample_idx])])\n","        sample_idx += 1\n","        col_idx += 1\n","        if col_idx == 4:\n","            col_idx = 0\n","            row_idx += 1\n","batch_img, batch_label = next(iter(train_dataloader))\n","show_samples(batch_img, batch_label, 8)"]},{"cell_type":"markdown","metadata":{},"source":["# vit"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.208542Z","iopub.status.idle":"2024-05-14T15:28:17.209188Z","shell.execute_reply":"2024-05-14T15:28:17.209020Z","shell.execute_reply.started":"2024-05-14T15:28:17.209003Z"},"trusted":true},"outputs":[],"source":["class PretrainViT(nn.Module):\n","\n","    def __init__(self):\n","        super(PretrainViT, self).__init__()\n","        model = models.vit_l_16(pretrained=True)\n","        num_classifier_feature = model.heads.head.in_features\n","        model.heads.head = nn.Sequential(\n","            nn.Linear(num_classifier_feature, 80)\n","        )\n","        self.model = model\n","\n","        for param in self.model.named_parameters():\n","            if \"heads\" not in param[0]:\n","                param[1].requires_grad = False\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.210222Z","iopub.status.idle":"2024-05-14T15:28:17.210704Z","shell.execute_reply":"2024-05-14T15:28:17.210518Z","shell.execute_reply.started":"2024-05-14T15:28:17.210502Z"},"trusted":true},"outputs":[],"source":["net = PretrainViT()\n","net.to(device)\n","print(f\"number of paramaters: {sum([param.numel() for param in net.parameters() if param.requires_grad])}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.212046Z","iopub.status.idle":"2024-05-14T15:28:17.212543Z","shell.execute_reply":"2024-05-14T15:28:17.212356Z","shell.execute_reply.started":"2024-05-14T15:28:17.212338Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.009)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.213658Z","iopub.status.idle":"2024-05-14T15:28:17.214051Z","shell.execute_reply":"2024-05-14T15:28:17.213863Z","shell.execute_reply.started":"2024-05-14T15:28:17.213847Z"},"trusted":true},"outputs":[],"source":["def get_accuracy(output, label):\n","    output = output.to(\"cpu\")\n","    label = label.to(\"cpu\")\n","\n","    sm = F.softmax(output, dim=1)\n","    _, index = torch.max(sm, dim=1)\n","    return torch.sum((label == index)) / label.size()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.215455Z","iopub.status.idle":"2024-05-14T15:28:17.215823Z","shell.execute_reply":"2024-05-14T15:28:17.215654Z","shell.execute_reply.started":"2024-05-14T15:28:17.215639Z"},"trusted":true},"outputs":[],"source":["def train(model, dataloader):\n","    model.train()\n","    running_loss = 0.0\n","    total_loss = 0.0\n","    running_acc = 0.0\n","    total_acc = 0.0\n","\n","    for batch_idx, (batch_img, batch_label) in enumerate(dataloader):\n","\n","        batch_img = batch_img.to(device)\n","        batch_label = batch_label.to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(batch_img)\n","        loss = criterion(output, batch_label)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        total_loss += loss.item()\n","\n","        acc = get_accuracy(output, batch_label)\n","        running_acc += acc\n","        total_acc += acc\n","\n","        if batch_idx % 100 == 0 and batch_idx != 0:\n","            print(f\"[step: {batch_idx:4d}/{len(dataloader)}] loss: {running_loss / 100:.3f}\")\n","            running_loss = 0.0\n","            running_acc = 0.0\n","    \n","    return total_loss / len(dataloader), total_acc / len(dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.217461Z","iopub.status.idle":"2024-05-14T15:28:17.217898Z","shell.execute_reply":"2024-05-14T15:28:17.217687Z","shell.execute_reply.started":"2024-05-14T15:28:17.217669Z"},"trusted":true},"outputs":[],"source":["def validate(model, dataloader):\n","    model.eval()\n","    total_loss = 0.0\n","    total_acc = 0.0\n","\n","    for batch_idx, (batch_img, batch_label) in enumerate(dataloader):\n","\n","        batch_img = batch_img.to(device)\n","        batch_label = batch_label.to(device)\n","\n","        # optimizer.zero_grad()\n","        output = model(batch_img)\n","        loss = criterion(output, batch_label)\n","        # loss.backward()\n","        # optimizer.step()\n","\n","        total_loss += loss.item()\n","        acc = get_accuracy(output, batch_label)\n","        total_acc += acc\n","    \n","    return total_loss / len(dataloader), total_acc / len(dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.219777Z","iopub.status.idle":"2024-05-14T15:28:17.220164Z","shell.execute_reply":"2024-05-14T15:28:17.219992Z","shell.execute_reply.started":"2024-05-14T15:28:17.219972Z"},"trusted":true},"outputs":[],"source":["class EarlyStopper:\n","    def __init__(self, patience=1, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.min_validation_loss = np.inf\n","\n","    def early_stop(self, validation_loss):\n","        if validation_loss < self.min_validation_loss:\n","            self.min_validation_loss = validation_loss\n","            self.counter = 0\n","        elif validation_loss > (self.min_validation_loss + self.min_delta):\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","        return False\n","early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.221319Z","iopub.status.idle":"2024-05-14T15:28:17.221722Z","shell.execute_reply":"2024-05-14T15:28:17.221549Z","shell.execute_reply.started":"2024-05-14T15:28:17.221533Z"},"trusted":true},"outputs":[],"source":["train_loss_history = []\n","valid_loss_history = []\n","train_acc_history = []\n","valid_acc_history = []\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","EPOCHS = 7\n","for epoch in range(EPOCHS):\n","    train_loss, train_acc = train(net, train_dataloader)\n","    valid_loss, valid_acc = validate(net, valid_dataloader)\n","    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n","\n","    train_loss_history.append(train_loss)\n","    valid_loss_history.append(valid_loss)\n","\n","    train_acc_history.append(train_acc)\n","    valid_acc_history.append(valid_acc)\n","\n","    if valid_loss <= min(valid_loss_history):\n","        torch.save(net.state_dict(), \"net.pt\")\n","    if early_stopper.early_stop(valid_loss):             \n","        break \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.224653Z","iopub.status.idle":"2024-05-14T15:28:17.225036Z","shell.execute_reply":"2024-05-14T15:28:17.224874Z","shell.execute_reply.started":"2024-05-14T15:28:17.224859Z"},"trusted":true},"outputs":[],"source":["\n","epochs = range(1,  len(train_loss_history)+1)\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","axes[0].plot(epochs, train_acc_history)\n","axes[0].plot(epochs, valid_acc_history)\n","axes[0].set_title('Vit Training and validation accuracy',\n","                  fontsize=12, fontweight='bold')\n","axes[0].set_ylabel('Accuracy')\n","axes[0].set_xlabel('epoch') \n","axes[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","axes[1].plot(epochs, train_loss_history)\n","axes[1].plot(epochs, valid_loss_history)\n","axes[1].set_title('Vit Training and validation loss',\n","                  fontsize=12, fontweight='bold')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_xlabel('epoch') \n","axes[1].legend(['Train', 'Validation'], loc='upper right')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 预训练MaxVit"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.226631Z","iopub.status.idle":"2024-05-14T15:28:17.226996Z","shell.execute_reply":"2024-05-14T15:28:17.226836Z","shell.execute_reply.started":"2024-05-14T15:28:17.226821Z"},"trusted":true},"outputs":[],"source":["del net,early_stopper"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.228851Z","iopub.status.idle":"2024-05-14T15:28:17.229216Z","shell.execute_reply":"2024-05-14T15:28:17.229055Z","shell.execute_reply.started":"2024-05-14T15:28:17.229040Z"},"trusted":true},"outputs":[],"source":["net = timm.create_model('maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k',\n","                          pretrained=True, drop_rate=0.2,\n","                         )\n","torch.set_grad_enabled(True)\n","num_classes = 80\n","\n","for param in net.parameters():\n","    param.requires_grad = False\n","\n","net.head.fc = nn.Linear(net.head.fc.in_features, num_classes)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","net.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.230622Z","iopub.status.idle":"2024-05-14T15:28:17.230994Z","shell.execute_reply":"2024-05-14T15:28:17.230831Z","shell.execute_reply.started":"2024-05-14T15:28:17.230816Z"},"trusted":true},"outputs":[],"source":["early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n","train_loss_history = []\n","valid_loss_history = []\n","train_acc_history = []\n","valid_acc_history = []\n","optimizer = optim.AdamW(net.parameters(), lr=0.0005)\n","def adjust_learning_rate(lr, epoch):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 0.5 every 2 epochs\"\"\"\n","    if epoch % 2 == 0 and epoch != 0:\n","        lr *= 0.5\n","    return lr\n","lr = 0.0005\n","EPOCHS = 7\n","for epoch in range(EPOCHS):\n","    lr = adjust_learning_rate(lr, epoch)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    train_loss, train_acc = train(net, train_dataloader)\n","    valid_loss, valid_acc = validate(net, valid_dataloader)\n","    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n","\n","    train_loss_history.append(train_loss)\n","    valid_loss_history.append(valid_loss)\n","\n","    train_acc_history.append(train_acc)\n","    valid_acc_history.append(valid_acc)\n","\n","    if valid_loss <= min(valid_loss_history):\n","        torch.save(net.state_dict(), \"maxvit1-net.pt\")\n","    if early_stopper.early_stop(valid_loss):      \n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.232401Z","iopub.status.idle":"2024-05-14T15:28:17.232769Z","shell.execute_reply":"2024-05-14T15:28:17.232603Z","shell.execute_reply.started":"2024-05-14T15:28:17.232588Z"},"trusted":true},"outputs":[],"source":["epochs = range(1,  len(train_loss_history)+1)\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","axes[0].plot(epochs, train_acc_history)\n","axes[0].plot(epochs, valid_acc_history)\n","axes[0].set_title('maxvit Training and validation accuracy',\n","                  fontsize=12, fontweight='bold')\n","axes[0].set_ylabel('Accuracy')\n","axes[0].set_xlabel('epoch') \n","axes[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","axes[1].plot(epochs, train_loss_history)\n","axes[1].plot(epochs, valid_loss_history)\n","axes[1].set_title('maxvit Training and validation loss',\n","                  fontsize=12, fontweight='bold')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_xlabel('epoch') \n","axes[1].legend(['Train', 'Validation'], loc='upper right')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 预训练maxvit训练最后三层"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.234073Z","iopub.status.idle":"2024-05-14T15:28:17.234530Z","shell.execute_reply":"2024-05-14T15:28:17.234278Z","shell.execute_reply.started":"2024-05-14T15:28:17.234263Z"},"trusted":true},"outputs":[],"source":["del net,early_stopper"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.235598Z","iopub.status.idle":"2024-05-14T15:28:17.235942Z","shell.execute_reply":"2024-05-14T15:28:17.235795Z","shell.execute_reply.started":"2024-05-14T15:28:17.235781Z"},"trusted":true},"outputs":[],"source":["net = timm.create_model('maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k',\n","                          pretrained=True, drop_rate=0.2,\n","                         )\n","torch.set_grad_enabled(True)\n","num_classes = 80\n","for param in net.parameters():\n","    param.requires_grad = False\n","\n","# model.stages[3] covers the last two blocks of the model\n","for param in net.stages[3].parameters():\n","    param.requires_grad = True\n","    \n","for param in net.head.parameters():\n","    param.requires_grad = True\n","\n","net.head.fc = nn.Linear(net.head.fc.in_features, num_classes)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","net.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.237648Z","iopub.status.idle":"2024-05-14T15:28:17.237985Z","shell.execute_reply":"2024-05-14T15:28:17.237837Z","shell.execute_reply.started":"2024-05-14T15:28:17.237823Z"},"trusted":true},"outputs":[],"source":["early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n","train_loss_history = []\n","valid_loss_history = []\n","train_acc_history = []\n","valid_acc_history = []\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","EPOCHS = 7\n","for epoch in range(EPOCHS):\n","    train_loss, train_acc = train(net, train_dataloader)\n","    valid_loss, valid_acc = validate(net, valid_dataloader)\n","    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n","\n","    train_loss_history.append(train_loss)\n","    valid_loss_history.append(valid_loss)\n","\n","    train_acc_history.append(train_acc)\n","    valid_acc_history.append(valid_acc)\n","\n","    if valid_loss <= min(valid_loss_history):\n","        torch.save(net.state_dict(), \"maxvit3-net.pt\")\n","    if early_stopper.early_stop(valid_loss):             \n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.239524Z","iopub.status.idle":"2024-05-14T15:28:17.239878Z","shell.execute_reply":"2024-05-14T15:28:17.239705Z","shell.execute_reply.started":"2024-05-14T15:28:17.239692Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["epochs = range(1,  len(train_loss_history)+1)\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","axes[0].plot(epochs, train_acc_history)\n","axes[0].plot(epochs, valid_acc_history)\n","axes[0].set_title('maxvit Training and validation accuracy',\n","                  fontsize=12, fontweight='bold')\n","axes[0].set_ylabel('Accuracy')\n","axes[0].set_xlabel('epoch') \n","axes[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","axes[1].plot(epochs, train_loss_history)\n","axes[1].plot(epochs, valid_loss_history)\n","axes[1].set_title('maxvit Training and validation loss',\n","                  fontsize=12, fontweight='bold')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_xlabel('epoch') \n","axes[1].legend(['Train', 'Validation'], loc='upper right')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# ResNet训练最后四层"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.241382Z","iopub.status.idle":"2024-05-14T15:28:17.241712Z","shell.execute_reply":"2024-05-14T15:28:17.241566Z","shell.execute_reply.started":"2024-05-14T15:28:17.241552Z"},"trusted":true},"outputs":[],"source":["del net,early_stopper"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.243118Z","iopub.status.idle":"2024-05-14T15:28:17.243546Z","shell.execute_reply":"2024-05-14T15:28:17.243375Z","shell.execute_reply.started":"2024-05-14T15:28:17.243346Z"},"trusted":true},"outputs":[],"source":["import torchvision\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","net = torchvision.models.resnet50(pretrained=True)\n","#num_classes = 70\n","in_features = net.fc.in_features\n","net.fc = torch.nn.Linear(in_features, num_classes)\n","for param in net.parameters():\n","    param.requires_grad = False\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","net.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.244722Z","iopub.status.idle":"2024-05-14T15:28:17.245101Z","shell.execute_reply":"2024-05-14T15:28:17.244926Z","shell.execute_reply.started":"2024-05-14T15:28:17.244911Z"},"trusted":true},"outputs":[],"source":["early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n","train_loss_history = []\n","valid_loss_history = []\n","train_acc_history = []\n","valid_acc_history = []\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","\n","EPOCHS = 7\n","for epoch in range(EPOCHS):\n","    train_loss, train_acc = train(net, train_dataloader)\n","    valid_loss, valid_acc = validate(net, valid_dataloader)\n","    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n","\n","    train_loss_history.append(train_loss)\n","    valid_loss_history.append(valid_loss)\n","\n","    train_acc_history.append(train_acc)\n","    valid_acc_history.append(valid_acc)\n","\n","    if valid_loss <= min(valid_loss_history):\n","        torch.save(net.state_dict(), \"reset4-net.pt\")\n","    if early_stopper.early_stop(valid_loss):             \n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.246565Z","iopub.status.idle":"2024-05-14T15:28:17.246930Z","shell.execute_reply":"2024-05-14T15:28:17.246769Z","shell.execute_reply.started":"2024-05-14T15:28:17.246753Z"},"trusted":true},"outputs":[],"source":["epochs = range(1,  len(train_loss_history)+1)\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","axes[0].plot(epochs, train_acc_history)\n","axes[0].plot(epochs, valid_acc_history)\n","axes[0].set_title('resnet50(4) Training and validation accuracy',\n","                  fontsize=12, fontweight='bold')\n","axes[0].set_ylabel('Accuracy')\n","axes[0].set_xlabel('epoch') \n","axes[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","axes[1].plot(epochs, train_loss_history)\n","axes[1].plot(epochs, valid_loss_history)\n","axes[1].set_title('resnet50(训练最后四层) Training and validation loss',\n","                  fontsize=12, fontweight='bold')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_xlabel('epoch') \n","axes[1].legend(['Train', 'Validation'], loc='upper right')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.247954Z","iopub.status.idle":"2024-05-14T15:28:17.248340Z","shell.execute_reply":"2024-05-14T15:28:17.248148Z","shell.execute_reply.started":"2024-05-14T15:28:17.248134Z"},"trusted":true},"outputs":[],"source":["del net,early_stopper"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.250251Z","iopub.status.idle":"2024-05-14T15:28:17.250640Z","shell.execute_reply":"2024-05-14T15:28:17.250481Z","shell.execute_reply.started":"2024-05-14T15:28:17.250466Z"},"trusted":true},"outputs":[],"source":["print(1)"]},{"cell_type":"markdown","metadata":{},"source":["# MBConv+vit"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.251758Z","iopub.status.idle":"2024-05-14T15:28:17.252129Z","shell.execute_reply":"2024-05-14T15:28:17.251948Z","shell.execute_reply.started":"2024-05-14T15:28:17.251934Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from einops.layers.torch import Rearrange\n","from timm.models.efficientnet_blocks import SqueezeExcite, DepthwiseSeparableConv\n","\n","from typing import Type\n","from torchvision import models\n","def _gelu_ignore_parameters(\n","        *args,\n","        **kwargs\n",") -> nn.Module:\n","    activation = nn.GELU()\n","    return activation\n","\n","# 定义 MBConv 模块\n","class MBConv(nn.Module):\n","\n","    def __init__(\n","            self,\n","            in_channels: int,\n","            out_channels: int,\n","            downscale: bool = False,\n","            act_layer: Type[nn.Module] = nn.GELU,\n","            norm_layer: Type[nn.Module] = nn.BatchNorm2d,\n","            drop_path: float = 0.,\n","    ) -> None:\n","        super(MBConv, self).__init__()\n","        self.drop_path_rate: float = drop_path\n","        if not downscale:\n","            assert in_channels == out_channels, \"If downscaling is utilized input and output channels must be equal.\"\n","        if act_layer == nn.GELU:\n","            act_layer = _gelu_ignore_parameters\n","        self.main_path = nn.Sequential(\n","            norm_layer(in_channels),\n","            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=(1, 1)),\n","            DepthwiseSeparableConv(in_chs=in_channels, out_chs=out_channels, stride=2 if downscale else 1,\n","                                   act_layer=act_layer, norm_layer=norm_layer, drop_path_rate=drop_path),\n","            SqueezeExcite(in_chs=out_channels, rd_ratio=0.25),\n","            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(1, 1))\n","        )\n","        self.skip_path = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1, 1))\n","        ) if downscale else nn.Identity()\n","\n","    def forward(\n","            self,\n","            input: torch.Tensor\n","    ) -> torch.Tensor:\n","        output = self.main_path(input)\n","        if self.drop_path_rate > 0.:\n","            output = drop_path(output, self.drop_path_rate, self.training)\n","        output = output + self.skip_path(input)\n","        return output\n","    \n","class con_vit(nn.Module):\n","    def __init__(self,num_classes):\n","        super(con_vit,self).__init__()\n","        self.mbconv = MBConv(in_channels= 3 ,out_channels= 3,downscale=False)\n","        vit16 = models.vit_l_16(pretrained=True)\n","        num_classifier_feature = vit16.heads.head.in_features\n","        vit16.heads.head = nn.Sequential(\n","            nn.Linear(num_classifier_feature, num_classes)\n","        )\n","        for param in vit16.named_parameters(): #对于所有的参数\n","            if \"heads\" not in param[0]:             #如果不是分类器头部的参数\n","                param[1].requires_grad = False\n","        self.vit = vit16\n","    def forward(self,x):\n","        x = self.mbconv(x)\n","        x = self.vit(x)\n","        return x\n","    \n","#num_classes = 70\n","net = con_vit(num_classes)\n","net.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.253706Z","iopub.status.idle":"2024-05-14T15:28:17.254070Z","shell.execute_reply":"2024-05-14T15:28:17.253911Z","shell.execute_reply.started":"2024-05-14T15:28:17.253896Z"},"trusted":true},"outputs":[],"source":["early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n","train_loss_history = []\n","valid_loss_history = []\n","train_acc_history = []\n","valid_acc_history = []\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","\n","EPOCHS = 7\n","for epoch in range(EPOCHS):\n","    train_loss, train_acc = train(net, train_dataloader)\n","    valid_loss, valid_acc = validate(net, valid_dataloader)\n","    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n","\n","    train_loss_history.append(train_loss)\n","    valid_loss_history.append(valid_loss)\n","\n","    train_acc_history.append(train_acc)\n","    valid_acc_history.append(valid_acc)\n","\n","    if valid_loss <= min(valid_loss_history):\n","        torch.save(net.state_dict(), \"net.pt\")\n","    if early_stopper.early_stop(valid_loss):      \n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.255324Z","iopub.status.idle":"2024-05-14T15:28:17.255686Z","shell.execute_reply":"2024-05-14T15:28:17.255527Z","shell.execute_reply.started":"2024-05-14T15:28:17.255512Z"},"trusted":true},"outputs":[],"source":["epochs = range(1,  len(train_loss_history)+1)\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","axes[0].plot(epochs, train_acc_history)\n","axes[0].plot(epochs, valid_acc_history)\n","axes[0].set_title('Convit Training and validation accuracy',\n","                  fontsize=12, fontweight='bold')\n","axes[0].set_ylabel('Accuracy')\n","axes[0].set_xlabel('epoch') \n","axes[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","axes[1].plot(epochs, train_loss_history)\n","axes[1].plot(epochs, valid_loss_history)\n","axes[1].set_title('Convit Training and validation loss',\n","                  fontsize=12, fontweight='bold')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_xlabel('epoch') \n","axes[1].legend(['Train', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Resnet50（只训练最后一层）"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.256609Z","iopub.status.idle":"2024-05-14T15:28:17.256965Z","shell.execute_reply":"2024-05-14T15:28:17.256806Z","shell.execute_reply.started":"2024-05-14T15:28:17.256792Z"},"trusted":true},"outputs":[],"source":["del net,early_stopper"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.257887Z","iopub.status.idle":"2024-05-14T15:28:17.258243Z","shell.execute_reply":"2024-05-14T15:28:17.258073Z","shell.execute_reply.started":"2024-05-14T15:28:17.258059Z"},"trusted":true},"outputs":[],"source":["net = timm.create_model('resnet50',\n","                          pretrained=True,\n","                          drop_rate=0.2,\n","                         )\n","print('Classifier layer:', net.get_classifier())\n","for param in net.parameters():\n","    param.requires_grad = False\n","#num_classes = 70\n","net.fc = nn.Linear(net.fc.in_features, num_classes)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","net.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.259229Z","iopub.status.idle":"2024-05-14T15:28:17.259625Z","shell.execute_reply":"2024-05-14T15:28:17.259465Z","shell.execute_reply.started":"2024-05-14T15:28:17.259450Z"},"trusted":true},"outputs":[],"source":["early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n","train_loss_history = []\n","valid_loss_history = []\n","train_acc_history = []\n","valid_acc_history = []\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","\n","EPOCHS = 7\n","for epoch in range(EPOCHS):\n","    train_loss, train_acc = train(net, train_dataloader)\n","    valid_loss, valid_acc = validate(net, valid_dataloader)\n","    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n","\n","    train_loss_history.append(train_loss)\n","    valid_loss_history.append(valid_loss)\n","\n","    train_acc_history.append(train_acc)\n","    valid_acc_history.append(valid_acc)\n","\n","    if valid_loss <= min(valid_loss_history):\n","        torch.save(net.state_dict(), \"resnet1-net.pt\")\n","    if early_stopper.early_stop(valid_loss):\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.260902Z","iopub.status.idle":"2024-05-14T15:28:17.261225Z","shell.execute_reply":"2024-05-14T15:28:17.261080Z","shell.execute_reply.started":"2024-05-14T15:28:17.261066Z"},"trusted":true},"outputs":[],"source":["epochs = range(1,  len(train_loss_history)+1)\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","axes[0].plot(epochs, train_acc_history)\n","axes[0].plot(epochs, valid_acc_history)\n","axes[0].set_title('resnet50(1) Training and validation accuracy',\n","                  fontsize=12, fontweight='bold')\n","axes[0].set_ylabel('Accuracy')\n","axes[0].set_xlabel('epoch') \n","axes[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","axes[1].plot(epochs, train_loss_history)\n","axes[1].plot(epochs, valid_loss_history)\n","axes[1].set_title('resnet50(训练最后一层) Training and validation loss',\n","                  fontsize=12, fontweight='bold')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_xlabel('epoch') \n","axes[1].legend(['Train', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# maxvit（自己写）"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.263061Z","iopub.status.idle":"2024-05-14T15:28:17.263454Z","shell.execute_reply":"2024-05-14T15:28:17.263258Z","shell.execute_reply.started":"2024-05-14T15:28:17.263243Z"},"trusted":true},"outputs":[],"source":["del net,early_stopper"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.265161Z","iopub.status.idle":"2024-05-14T15:28:17.265543Z","shell.execute_reply":"2024-05-14T15:28:17.265380Z","shell.execute_reply.started":"2024-05-14T15:28:17.265364Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import os\n","import glob\n","from PIL import Image\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset, Subset, DataLoader\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from PIL import Image\n","import os\n","\n","from re import T\n","from typing import Type, Callable, Tuple, Optional, Set, List, Union\n","\n","from matplotlib.dates import relativedelta\n","from matplotlib.pyplot import grid\n","from numpy import intp\n","import torch\n","import torch.nn as nn\n","\n","from timm.models.efficientnet_blocks import SqueezeExcite, DepthwiseSeparableConv\n","from timm.models.layers import drop_path, trunc_normal_, Mlp, DropPath\n","\n","\n","def _gelu_ignore_parameters(*args, **kwargs) -> nn.Module:\n","    activation = nn.GELU()\n","    return activation\n","\n","\n","class MBConv(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        out_channels: int,\n","        downscale: bool = False,\n","        act_layer: Type[nn.Module] = nn.GELU,\n","        norm_layer: Type[nn.Module] = nn.BatchNorm2d,\n","        drop_path: float = 0.0,\n","    ):\n","        super(MBConv, self).__init__()\n","        self.drop_path_rate: float = drop_path\n","        if not downscale:\n","            assert in_channels == out_channels\n","        if act_layer == nn.GELU:\n","            act_layer = _gelu_ignore_parameters  # type: ignore\n","        self.main_path = nn.Sequential(\n","            norm_layer(in_channels),\n","            nn.Conv2d(\n","                in_channels=in_channels, out_channels=in_channels, kernel_size=(1, 1)\n","            ),\n","            DepthwiseSeparableConv(\n","                in_chs=in_channels,\n","                out_chs=out_channels,\n","                stride=2 if downscale else 1,\n","                act_layer=act_layer,  # type: ignore\n","                norm_layer=norm_layer,  # type:ignore\n","                drop_path_rate=drop_path,\n","            ),\n","            SqueezeExcite(in_chs=out_channels, rd_ratio=0.25),\n","            nn.Conv2d(\n","                in_channels=out_channels, out_channels=out_channels, kernel_size=(1, 1)\n","            ),\n","        )\n","        self.skip_path = (\n","            nn.Sequential(\n","                nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n","                nn.Conv2d(\n","                    in_channels=in_channels,\n","                    out_channels=out_channels,\n","                    kernel_size=(1, 1),\n","                ),\n","            )\n","            if downscale\n","            else nn.Identity()\n","        )\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        output = self.main_path(input)\n","        if self.drop_path_rate > 0.0:\n","            output = drop_path(output, self.drop_path_rate, self.training)\n","        output = output + self.skip_path(input)\n","        return output\n","\n","\n","def window_partition(\n","    input: torch.Tensor, window_size: Tuple[int, int] = (7, 7)\n",") -> torch.Tensor:\n","    B, C, H, W = input.shape\n","    windows = input.view(\n","        B, C, H // window_size[0], window_size[0], W // window_size[1], window_size[1]\n","    )\n","    windows = (\n","        windows.permute(0, 2, 4, 3, 5, 1)\n","        .contiguous()\n","        .view(-1, window_size[0], window_size[1], C)\n","    )\n","    return windows\n","\n","\n","def window_reverse(\n","    windows: torch.Tensor,\n","    original_size: Tuple[int, int],\n","    window_size: Tuple[int, int] = (7, 7),\n",") -> torch.Tensor:\n","    H, W = original_size\n","    B = int(windows.shape[0] / (H * W / window_size[0] / window_size[1]))\n","    output = windows.view(\n","        B, H // window_size[0], W // window_size[1], window_size[0], window_size[1], -1\n","    )\n","    output = output.permute(0, 5, 1, 3, 2, 4).contiguous().view(B, -1, H, W)\n","    return output\n","\n","\n","def grid_partition(\n","    input: torch.Tensor, grid_size: Tuple[int, int] = (7, 7)\n",") -> torch.Tensor:\n","    B, C, H, W = input.shape\n","    grid = input.view(\n","        B, C, grid_size[0], H // grid_size[0], grid_size[1], W // grid_size[1]\n","    )\n","    grid = (\n","        grid.permute(0, 3, 5, 2, 4, 1)\n","        .contiguous()\n","        .view(-1, grid_size[0], grid_size[1], C)\n","    )\n","    return grid\n","\n","\n","def grid_reverse(\n","    grid: torch.Tensor,\n","    original_size: Tuple[int, int],\n","    grid_size: Tuple[int, int] = (7, 7),\n",") -> torch.Tensor:\n","    (H, W), C = original_size, grid.shape[-1]\n","    B = int(grid.shape[0] / (H * W / grid_size[0] / grid_size[1]))\n","    output = grid.view(\n","        B, H // grid_size[0], W // grid_size[1], grid_size[0], grid_size[1], C\n","    )\n","    output = output.permute(0, 5, 3, 1, 4, 2).contiguous().view(B, C, H, W)\n","    return output\n","\n","\n","def get_relative_position_index(win_h: int, win_w: int) -> torch.Tensor:\n","    coords = torch.stack(torch.meshgrid([torch.arange(win_h), torch.arange(win_w)]))\n","    coords_flatten = torch.flatten(coords, 1)\n","    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","    relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","    relative_coords[:, :, 0] += win_h - 1\n","    relative_coords[:, :, 1] += win_w - 1\n","    relative_coords[:, :, 0] *= 2 * win_w - 1\n","    return relative_coords.sum(-1)\n","\n","\n","class RelativeSelfAttention(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        num_heads: int = 32,\n","        grid_window_size: Tuple[int, int] = (7, 7),\n","        attn_drop: float = 0.0,\n","        drop: float = 0.0,\n","    ):\n","        super(RelativeSelfAttention, self).__init__()\n","        self.in_channels: int = in_channels\n","        self.num_heads: int = num_heads\n","        self.grid_window_size: Tuple[int, int] = grid_window_size\n","        self.scale: float = num_heads**-0.5\n","        self.attn_area: int = grid_window_size[0] * grid_window_size[1]\n","        self.qkv_mapping = nn.Linear(\n","            in_features=in_channels, out_features=3 * in_channels, bias=True\n","        )\n","        self.attn_drop = nn.Dropout(p=attn_drop)\n","        self.proj = nn.Linear(\n","            in_features=in_channels, out_features=in_channels, bias=True\n","        )\n","        self.proj_drop = nn.Dropout(p=drop)\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros(\n","                (2 * grid_window_size[0] - 1) * (2 * grid_window_size[1] - 1), num_heads\n","            )\n","        )\n","        self.register_buffer(\n","            \"relative_position_index\",\n","            get_relative_position_index(grid_window_size[0], grid_window_size[1]),\n","        )\n","        trunc_normal_(self.relative_position_bias_table, std=0.02)\n","\n","    def _get_relative_position_bias(self) -> torch.Tensor:\n","        relative_position_bias = self.relative_position_bias_table[\n","            self.relative_position_index.view(-1)\n","        ].view(self.attn_area, self.attn_area, -1)\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n","        return relative_position_bias.unsqueeze(0)\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        B_, N, C = input.shape\n","        qkv = (\n","            self.qkv_mapping(input)\n","            .reshape(B_, N, 3, self.num_heads, -1)\n","            .permute(2, 0, 3, 1, 4)\n","        )\n","        q, k, v = qkv.unbind(0)\n","        attn = self.softmax(\n","            q @ k.transpose(-2, -1) + self._get_relative_position_bias()\n","        )\n","        output = (attn @ v).transpose(1, 2).reshape(B_, N, -1)\n","        output = self.proj(output)\n","        output = self.proj_drop(output)\n","        return output\n","\n","\n","class MaxViTTransformerBlock(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        partition_function: Callable,\n","        reverse_function: Callable,\n","        num_heads: int = 32,\n","        grid_window_size: Tuple[int, int] = (7, 7),\n","        attn_drop: float = 0.0,\n","        drop: float = 0.0,\n","        drop_path: float = 0.0,\n","        mlp_ratio: float = 4.0,\n","        act_layer: Type[nn.Module] = nn.GELU,\n","        norm_layer: Type[nn.Module] = nn.LayerNorm,\n","    ) -> None:\n","        super(MaxViTTransformerBlock, self).__init__()\n","        self.partition_function: Callable = partition_function\n","        self.reverse_function: Callable = reverse_function\n","        self.grid_window_size: Tuple[int, int] = grid_window_size\n","        self.norm_1 = norm_layer(in_channels)\n","        self.attention = RelativeSelfAttention(\n","            in_channels=in_channels,\n","            num_heads=num_heads,\n","            grid_window_size=grid_window_size,\n","            attn_drop=attn_drop,\n","            drop=drop,\n","        )\n","        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n","        self.norm_2 = norm_layer(in_channels)\n","        self.mlp = Mlp(\n","            in_features=in_channels,\n","            hidden_features=int(mlp_ratio * in_channels),\n","            act_layer=act_layer,  # type:ignore\n","            drop=drop,\n","        )\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        B, C, H, W = input.shape\n","        input_partitioned = self.partition_function(input, self.grid_window_size)\n","        input_partitioned = input_partitioned.view(\n","            -1, self.grid_window_size[0] * self.grid_window_size[1], C\n","        )\n","        output = input_partitioned + self.drop_path(\n","            self.attention(self.norm_1(input_partitioned))\n","        )\n","        output = output + self.drop_path(self.mlp(self.norm_2(output)))\n","        output = self.reverse_function(output, (H, W), self.grid_window_size)\n","        return output\n","\n","\n","class MaxViTBlock(nn.Module):\n","\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        out_channels: int,\n","        downscale: bool = False,\n","        num_heads: int = 32,\n","        grid_window_size: Tuple[int, int] = (7, 7),\n","        attn_drop: float = 0.0,\n","        drop: float = 0.0,\n","        drop_path: float = 0.0,\n","        mlp_ratio: float = 4.0,\n","        act_layer: Type[nn.Module] = nn.GELU,\n","        norm_layer: Type[nn.Module] = nn.BatchNorm2d,\n","        norm_layer_transformer: Type[nn.Module] = nn.LayerNorm,\n","    ) -> None:\n","        super(MaxViTBlock, self).__init__()\n","        self.mb_conv = MBConv(\n","            in_channels=in_channels,\n","            out_channels=out_channels,\n","            downscale=downscale,\n","            act_layer=act_layer,\n","            norm_layer=norm_layer,\n","            drop_path=drop_path,\n","        )\n","        self.block_transformer = MaxViTTransformerBlock(\n","            in_channels=out_channels,\n","            partition_function=window_partition,\n","            reverse_function=window_reverse,\n","            num_heads=num_heads,\n","            grid_window_size=grid_window_size,\n","            attn_drop=attn_drop,\n","            drop=drop,\n","            drop_path=drop_path,\n","            mlp_ratio=mlp_ratio,\n","            act_layer=act_layer,\n","            norm_layer=norm_layer_transformer,\n","        )\n","        self.grid_transformer = MaxViTTransformerBlock(\n","            in_channels=out_channels,\n","            partition_function=grid_partition,\n","            reverse_function=grid_reverse,\n","            num_heads=num_heads,\n","            grid_window_size=grid_window_size,\n","            attn_drop=attn_drop,\n","            drop=drop,\n","            drop_path=drop_path,\n","            mlp_ratio=mlp_ratio,\n","            act_layer=act_layer,\n","            norm_layer=norm_layer_transformer,\n","        )\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        output = self.grid_transformer(self.block_transformer(self.mb_conv(input)))\n","        return output\n","\n","\n","class MaxViTStage(nn.Module):\n","\n","    def __init__(\n","        self,\n","        depth: int,\n","        in_channels: int,\n","        out_channels: int,\n","        num_heads: int = 32,\n","        grid_window_size: Tuple[int, int] = (7, 7),\n","        attn_drop: float = 0.0,\n","        drop: float = 0.0,\n","        drop_path: Union[List[float], float] = 0.0,\n","        mlp_ratio: float = 4.0,\n","        act_layer: Type[nn.Module] = nn.GELU,\n","        norm_layer: Type[nn.Module] = nn.BatchNorm2d,\n","        norm_layer_transformer: Type[nn.Module] = nn.LayerNorm,\n","    ) -> None:\n","        super(MaxViTStage, self).__init__()\n","        self.blocks = nn.Sequential(\n","            *[\n","                MaxViTBlock(\n","                    in_channels=in_channels if index == 0 else out_channels,\n","                    out_channels=out_channels,\n","                    downscale=index == 0,\n","                    num_heads=num_heads,\n","                    grid_window_size=grid_window_size,\n","                    attn_drop=attn_drop,\n","                    drop=drop,\n","                    drop_path=(\n","                        drop_path if isinstance(drop_path, float) else drop_path[index]  # type: ignore\n","                    ),\n","                    mlp_ratio=mlp_ratio,\n","                    act_layer=act_layer,\n","                    norm_layer=norm_layer,\n","                    norm_layer_transformer=norm_layer_transformer,\n","                )\n","                for index in range(depth)\n","            ]\n","        )\n","\n","    def forward(self, input=torch.Tensor) -> torch.Tensor:\n","        output = self.blocks(input)\n","        return output\n","\n","\n","class MaxViT(nn.Module):\n","\n","    def __init__(\n","        self,\n","        in_channels: int = 3,\n","        depths: Tuple[int, ...] = (2, 2, 5, 2),\n","        channels: Tuple[int, ...] = (64, 128, 256, 512),\n","        num_classes: int = 1000,\n","        embed_dim: int = 64,\n","        num_heads: int = 32,\n","        grid_window_size: Tuple[int, int] = (7, 7),\n","        attn_drop: float = 0.0,\n","        drop=0.0,\n","        drop_path=0.0,\n","        mlp_ratio=4.0,\n","        act_layer=nn.GELU,\n","        norm_layer=nn.BatchNorm2d,\n","        norm_layer_transformer=nn.LayerNorm,\n","        global_pool: str = \"avg\",\n","    ) -> None:\n","        super(MaxViT, self).__init__()\n","        # Check parameters\n","        assert len(depths) == len(\n","            channels\n","        ), \"For each stage a channel dimension must be given.\"\n","        assert global_pool in [\n","            \"avg\",\n","            \"max\",\n","        ], f\"Only avg and max is supported but {global_pool} is given\"\n","        # Save parameters\n","        self.num_classes: int = num_classes\n","        # Init convolutional stem\n","        self.stem = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=in_channels,\n","                out_channels=embed_dim,\n","                kernel_size=(3, 3),\n","                stride=(2, 2),\n","                padding=(1, 1),\n","            ),\n","            act_layer(),\n","            nn.Conv2d(\n","                in_channels=embed_dim,\n","                out_channels=embed_dim,\n","                kernel_size=(3, 3),\n","                stride=(1, 1),\n","                padding=(1, 1),\n","            ),\n","            act_layer(),\n","        )\n","        # Init blocks\n","        drop_path = torch.linspace(0.0, drop_path, sum(depths)).tolist()\n","        stages = []\n","        for index, (depth, channel) in enumerate(zip(depths, channels)):\n","            stages.append(\n","                MaxViTStage(\n","                    depth=depth,\n","                    in_channels=embed_dim if index == 0 else channels[index - 1],\n","                    out_channels=channel,\n","                    num_heads=num_heads,\n","                    grid_window_size=grid_window_size,\n","                    attn_drop=attn_drop,\n","                    drop=drop,\n","                    drop_path=drop_path[sum(depths[:index]) : sum(depths[: index + 1])],\n","                    mlp_ratio=mlp_ratio,\n","                    act_layer=act_layer,\n","                    norm_layer=norm_layer,\n","                    norm_layer_transformer=norm_layer_transformer,\n","                )\n","            )\n","        self.stages = nn.ModuleList(stages)\n","        self.global_pool: str = global_pool\n","        self.head = nn.Linear(channels[-1], num_classes)\n","\n","    @torch.jit.ignore  # type: ignore\n","    def no_weight_decay(self) -> Set[str]:\n","        nwd = set()\n","        for n, _ in self.named_parameters():\n","            if \"relative_position_bias_table\" in n:\n","                nwd.add(n)\n","        return nwd\n","\n","    def reset_classifier(\n","        self, num_classes: int, global_pool: Optional[str] = None\n","    ) -> None:\n","        self.num_classes: int = num_classes\n","        if global_pool is not None:\n","            self.global_pool = global_pool\n","        self.head = (\n","            nn.Linear(self.num_features, num_classes)\n","            if num_classes > 0\n","            else nn.Identity()\n","        )\n","\n","    def forward_features(self, input: torch.Tensor) -> torch.Tensor:\n","        output = input\n","        for stage in self.stages:\n","            output = stage(output)\n","        return output\n","\n","    def forward_head(self, input: torch.Tensor, pre_logits: bool = False):\n","        if self.global_pool == \"avg\":\n","            input = input.mean(dim=(2, 3))\n","        elif self.global_pool == \"max\":\n","            input = torch.amax(input, dim=(2, 3))\n","        return input if pre_logits else self.head(input)\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        output = self.forward_features(self.stem(input))\n","        output = self.forward_head(output)\n","        return output\n","\n","\n","def max_vit_base_224(**kwargs) -> MaxViT:\n","    \"\"\"MaxViT base for a resolution of 224 X 224\"\"\"\n","    return MaxViT(\n","        depths=(2, 6, 14, 2), channels=(96, 192, 384, 768), embed_dim=64, **kwargs\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.266688Z","iopub.status.idle":"2024-05-14T15:28:17.267013Z","shell.execute_reply":"2024-05-14T15:28:17.266860Z","shell.execute_reply.started":"2024-05-14T15:28:17.266847Z"},"trusted":true},"outputs":[],"source":["early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n","optimizer = optim.SGD(net.parameters(), lr=0.015)\n","net.to(device)\n","train_loss_history = []\n","valid_loss_history = []\n","train_acc_history = []\n","valid_acc_history = []\n","EPOCHS = 20\n","for epoch in range(EPOCHS):\n","    train_loss, train_acc = train(net, train_dataloader)\n","    valid_loss, valid_acc = validate(net, valid_dataloader)\n","    print(\n","        f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\"\n","    )\n","    train_loss_history.append(train_loss)\n","    valid_loss_history.append(valid_loss)\n","\n","    train_acc_history.append(train_acc)\n","    valid_acc_history.append(valid_acc)\n","\n","    if valid_loss <= min(valid_loss_history):\n","        torch.save(net.state_dict(), \"maxvit-net.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-14T15:28:17.273962Z","iopub.status.idle":"2024-05-14T15:28:17.274400Z","shell.execute_reply":"2024-05-14T15:28:17.274183Z","shell.execute_reply.started":"2024-05-14T15:28:17.274167Z"},"trusted":true},"outputs":[],"source":["epochs = range(1,  len(train_loss_history)+1)\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","axes[0].plot(epochs, train_acc_history)\n","axes[0].plot(epochs, valid_acc_history)\n","axes[0].set_title('resnet50(1) Training and validation accuracy',\n","                  fontsize=12, fontweight='bold')\n","axes[0].set_ylabel('Accuracy')\n","axes[0].set_xlabel('epoch') \n","axes[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","axes[1].plot(epochs, train_loss_history)\n","axes[1].plot(epochs, valid_loss_history)\n","axes[1].set_title('resnet50(训练最后一层) Training and validation loss',\n","                  fontsize=12, fontweight='bold')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_xlabel('epoch') \n","axes[1].legend(['Train', 'Validation'], loc='upper right')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":534640,"sourceId":5468571,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}
